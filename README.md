# 智能语音助手

## 如何运行程序
1. **拉取依赖**
   ```bash
   go mod tidy
2. 准备数据库
   - 创建数据库（示例名：assistant_voice）：
    ```
    CREATE DATABASE assistant_voice CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
    ```
   - 记下 MySQL 的「主机/IP、端口、用户名、密码」。

3. 配置 MySQL 连接
   - 打开 db.mysql.connect.go，设置 DSN（把下面示例替换为你自己的信息）：
    ```
   dsn := fmt.Sprintf("用户名:密码@tcp(127.0.0.1:3306)/assistant_voice?charset=utf8mb4&parseTime=True&loc=Local")
   ```
4. 配置 Redis
   - 打开 cache.redis.connect.go，设置 Redis 地址（如有密码，按你的代码支持方式添加）：
    ```
   redisHost = "127.0.0.1:6379"
   ```
5. 运行
    ```bash
    go run main.go
    ```
   - 启动成功后，访问：
    ```bash
   http://localhost:8080/user/login
   ```
    
## 架构设计文档
### 程序用途
本系统旨在构建一个 基于语音对话的大模型智能桌面控制系统，用户可以通过自然语言语音指令与电脑进行交互，实现从简单到复杂的操作任务。

系统通过语音识别（ASR）、语义理解（LLM）和本地执行（Action Manager）三个核心环节，实现“听得懂、能理解、会操作”的智能语音助手。

| 目标类别         | 描述                                                |
| ------------ |---------------------------------------------------|
| 🗣️ 自然语音交互   | 用户可通过麦克风直接下达语音指令，如“播放音乐”、“帮我写一篇报告”、“打开浏览器”。       |
| 🧠 大模型语义理解   | 使用大语言模型（LLM）理解用户意图，分析上下文，实现多轮对话与任务规划。             |
| ⚙️ 本地应用控制    | 系统可启动、关闭、操作本地应用程序（如浏览器、记事本、音乐播放器）。                |
| 🪄 能力组合与任务编排 | LLM 能根据上下文组合多个系统能力，执行复杂任务（如“保存刚写的文档并用notepad打开”）。 |
| 📡 语音输入与反馈   | 支持语音输入与语音播报反馈，实现全语音闭环交互。                          |
### 系统架构  
![img_1.png](img_1.png)

### 模块设计

| 模块                 | 功能               | 关键技术                          | 接口方式      |
| ------------------ | ---------------- |-------------------------------| --------- |
| **ASRProvider**    | 语音转文本            | FunASR / Whisper              | 文本输出      |
| **LLMController**  | 调用大模型理解意图        | Qwen3                         | JSON 指令结构 |
| **TaskPlanner**    | 任务规划与组合          | ReAct 思维链                     | 动作列表      |
| **ActionExecutor** | 执行任务（打开/关闭/操作程序） | Go + exec.Command / Robot API | 执行状态      |
| **AppManager**     | 管理本地应用状态         | Windows API / psutils         | PID 管理    |
| **TTSProvider**    | 文本转语音播报          | Edge-TTS / pyttsx3 / Coqui    | 音频播放      |

### 关于开发流程
由于本项目为单人开发项目，为了保证快速迭代与持续集成效率，当前阶段采用直接推送到 main 分支的工作流。
然而，在多人协作或企业级项目环境中，我们将严格遵循 Pull Request + Code Review 的协作模式，以保障代码质量、规范性与可维护性。
## 相关问题
### 你认为这个产品需要哪些功能？这些功能各自的优先级是什么？你计划本次开发哪些功能？
| 功能模块                           | 功能描述                      | 优先级  | 说明                      |
| ------------------------------ | ------------------------- | ---- | ----------------------- |
| 🎤 **语音识别（ASR）**               | 将用户语音实时转写为文本，用于语义理解       | 🟥 高 | 属于整个系统的输入入口，没有它无法对话     |
| 🧠 **语义理解（LLMController）**     | 通过大语言模型理解用户意图，生成任务计划      | 🟥 高 | 核心智能模块，负责“理解用户说了什么”     |
| ⚙️ **动作执行（ActionExecutor）**    | 执行系统操作，如打开应用、输入文字、点击等     | 🟥 高 | 用户可见的主要反馈环节，是系统“能行动”的关键 |
| 📦 **应用管理（AppManager）**        | 启动、关闭、追踪应用的 PID 状态        | 🟧 中 | 提升执行稳定性，方便多任务调度与资源管理    |
| 🔊 **语音合成（TTSProvider）**       | 将文字转为语音播报，让系统具备语音反馈能力     | 🟧 中 | 提升人机交互体验，非关键但极具沉浸感      |
| 🪄 **任务编排（TaskPlanner）**       | 支持多步任务组合（例如“打开记事本并写入一句话”） | 🟨 中 | 提升系统智能程度，适合后期强化         |
| 🧩 **插件能力扩展（Skill/Action 扩展）** | 引入可插拔的应用能力（如天气、文件搜索、系统控制） | 🟩 低 | 可在基础系统稳定后扩展更多能力         |
✅ 本次开发计划实现功能：

1. ASRProvider（语音识别 → FunASR）

2. LLMController（意图识别 → Qwen3）

3. ActionExecutor（动作执行）

4. AppManager（应用启动与关闭）

5. 基础 Web 接口与配置系统（go + Gin 框架）
### 你认为这个产品实现上的挑战是什么，你计划如何应对这些挑战？
| 挑战点                  | 说明                        | 应对策略                                        |
| -------------------- | ------------------------- |---------------------------------------------|
| 🎧 **语音识别准确率与实时性**   | 本地识别模型延迟或精度不足可能导致体验卡顿     | 优先使用 **FunASR + Silero VAD** 实现端侧快速语音裁剪与识别  |
| 🧠 **LLM 指令结构化输出**   | 大模型输出易偏离格式，难以直接执行         | 通过 **JSON Schema + Few-shot 提示模板** 约束模型输出结构 |
| ⚙️ **系统命令执行安全性**     | 执行系统命令存在风险，如误关闭进程         | 引入 **白名单机制**，仅允许特定命令或应用被调用                  |
| 🪟 **Windows 环境兼容性** | Windows 不支持 robotgo 的部分功能 | 改用 **os/exec + PowerShell 命令** 实现启动/关闭与输入操作 |
| 🔁 **多轮语义上下文管理**     | 复杂指令可能需要上下文记忆             | 通过 **LLM Token** 回填方式实现                     |
| 🔊 **语音合成延迟问题**      | 实时语音播报可能卡顿                | 使用 **Edge-TTS** 支持异步播放与缓存语音片段               |

### 你计划采纳哪家公司的哪个 LLM 模型能力？你对比了哪些，你为什么选择用该 LLM 模型？
| 模型             | 提供方       | 特点                                  | 选择结果     |
| -------------- | --------- | ----------------------------------- | -------- |
| **Qwen3**      | 阿里巴巴      | 具备较强中文语义理解、命令生成能力；本地可部署；兼容 RKLLM 平台 | ✅ **选用** |
| GPT-4o         | OpenAI    | 推理能力强但需联网，成本较高                      | ❌        |
| Claude 3.5     | Anthropic | 推理稳健但国内可访问性低                        | ❌        |
| Yi-Large       | 零一万物      | 开源模型，语义不错但工具调用能力弱                   | ⚪ 备选     |
| DeepSeek-Coder | 深度求索      | 适合代码生成，但不适合语音交互                     | ⚪ 备选     |

### 你对这个产品有哪些未来规划中的功能？你为何觉得这些能力是重要的？
| 规划功能                                    | 功能描述                    | 重要性  |
| --------------------------------------- | ----------------------- | ---- |
| 🗂️ **插件系统（Skill Hub）**                 | 允许动态加载外部能力，如天气、时间表、网页搜索 | ⭐⭐⭐⭐ |
| 💬 **多轮对话记忆系统**                         | 通过 Redis + 向量数据库维护上下文   | ⭐⭐⭐⭐ |
| 🪄 **自动任务编排器（TaskGraph）**               | 让模型自动组合多步指令完成复杂任务       | ⭐⭐⭐⭐ |
| 📱 **跨平台支持（Windows / Linux / Android）** | 扩展到移动与边缘设备控制            | ⭐⭐⭐  |
| 🌐 **Web 控制台与语音面板**                     | 用浏览器监控、配置与可视化交互         | ⭐⭐   |
| 🔊 **情感语音反馈**                           | 根据任务情境生成拟人语音风格          | ⭐⭐   |
这些功能的重要性在于：

- 提升智能体的自主性与泛化能力（能组合任务）

- 增强人机交互体验（记忆、语音反馈）

- 为后续生态扩展（Skill 插件）奠定开放基础